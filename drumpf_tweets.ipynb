{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic scikit learn \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#from sklearn import datasets, linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "# plotting \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# nltk \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as sia \n",
    "\n",
    "# others \n",
    "import csv \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import random\n",
    "import collections\n",
    "import string \n",
    "import re\n",
    "\n",
    "import datetime\n",
    "from utils import *  # imports things like feature_engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features to use : \n",
    "\n",
    "- sentiment analysis for the text - positive or negative using nltk \n",
    "- time of the day \n",
    "- length of tweet \n",
    "- mention of I /#realDonaldTrump\n",
    "- number of hashtags\n",
    "- starts with \"@\n",
    "- number of links/retweets\n",
    "- the most common @ and links from Donald Trump vs the most common @ from his staff, see if there is a pattern \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['Dates'] = pd.to_datetime(data['created']).dt.date\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['Time'] = pd.to_datetime(data['created']).dt.time\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['hour'] = data['Time'].astype('str').str.split(':').apply (lambda x: int(x[0]) + int(x[1])/60.0)\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['newdate'] = pd.to_datetime(data['Dates']).dt.strftime('%y%j')\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['hashs'] = gethashs(data['text'])\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['links'] = getlinks(data['text'])\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['ats'] = getats(data['text'])\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['nhashs'] = [len(x) for x in data['hashs']]\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['nlinks'] = [len(x) for x in data['links']]\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['nats'] = [len(x) for x in data['ats']]\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['cleantext'] = data['text'].apply(lambda x: ' '.join([w for w in x.split() if (\"https:\" not in w and \"@\" not in w and \"#\" not in w)]))\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['cleantext'] = data['cleantext'].apply(lambda x: ' '.join([w for w in x.split() if ( \"&amp\" not in w)]))\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['cleantext'] = data['cleantext'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['ncaps'] = data['cleantext'].apply(lambda x: len([w for w in x.split() if w.isupper()]))\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['length'] = data['text'].apply(lambda x: len(x.split()))\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['nself'] = data['text'].apply(lambda x: len([w for w in x.split() if 'realDonaldTrump' in w ]))\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['ncampaign'] = data['text'].apply(lambda x: len([w for w in x.split() if 'Trump2016' in w ]))\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['rt'] = data['text'].apply(lambda x : len([w for w in x.split() if '\\\"@' in w ]))\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['pos'] = data['cleantext'].apply(lambda x: sia_init.polarity_scores(x)['pos']**power)\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['neg'] = data['cleantext'].apply(lambda x: sia_init.polarity_scores(x)['neg']**power)\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['neu'] = data['cleantext'].apply(lambda x: sia_init.polarity_scores(x)['neu']**power)\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['cleantext'] = data['cleantext'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
      "/Users/avani/Dropbox/Work/dataScience/CS5780/final/utils.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['cleantext'] = data['cleantext'].apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n"
     ]
    }
   ],
   "source": [
    "alldat = pd.read_csv('./all/train.csv')\n",
    "data_train_orig = alldat\n",
    "\n",
    "# shuffling the tweets around and selecting a fraction for holdout testing if needed; default is 1. \n",
    "\n",
    "frac = 1\n",
    "\n",
    "lim = int(data_train_orig.shape[0]*frac)\n",
    "random_indxs = np.random.permutation(len(data_train_orig))\n",
    "indxs = random_indxs[0:lim]\n",
    "indxs_secret = random_indxs[lim:]\n",
    "\n",
    "data_train = data_train_orig.iloc[indxs]; \n",
    "labels_train = alldat['label'].iloc[indxs]; \n",
    "\n",
    "data_train_secret = data_train_orig.iloc[indxs_secret]; \n",
    "labels_train_secret = alldat['label'].iloc[indxs_secret]; \n",
    "\n",
    "data_test =  pd.read_csv('./all/test.csv');\n",
    "\n",
    "## cleaning up the data \n",
    "\n",
    "data_train = feature_engineering(data_train, power=1); \n",
    "data_train_secret = feature_engineering(data_train_secret, power=1); \n",
    "data_test = feature_engineering(data_test, power=1); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['hour', 'length','nhashs', 'nlinks','nats',  'pos', 'neg', 'neu', 'nself', 'rt', 'ncampaign']\n",
    "\n",
    "X_train=data_train[feature_list]  # Features\n",
    "y_train=data_train['label'] # Labels\n",
    "\n",
    "if frac<1 : \n",
    "    \n",
    "    X_secret=data_train_secret[feature_list]  # Features\n",
    "    y_secret=data_train_secret['label'] # Labels\n",
    "\n",
    "X_test=data_test[feature_list]  # Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88 (+/- 0.03) [Adaboost]\n",
      "Accuracy: 0.89 (+/- 0.02) [Random Forest]\n",
      "Accuracy: 0.88 (+/- 0.02) [Gradient Boost]\n",
      "Accuracy: 0.90 (+/- 0.02) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "clf_ada = AdaBoostClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "\n",
    "clf_rfc=RandomForestClassifier(n_estimators=500).fit(X_train, y_train)\n",
    "\n",
    "clf_gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                     max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('abc', clf_ada), ('rf', clf_rfc), ('gbc', clf_gbc)], voting='soft').fit(X_train, y_train) # ,voting='soft', weights=[2, 1, 2]\n",
    "\n",
    "for clf, label in zip([clf_ada, clf_rfc, clf_gbc, eclf], ['Adaboost', 'Random Forest', 'Gradient Boost', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'rf__n_estimators': [100, 500, 1000],\n",
    "          'abc__n_estimators':[100, 500, 1000], \n",
    "         'gbc__n_estimators':[100, 500, 1000]}\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc__n_estimators': 100, 'gbc__n_estimators': 100, 'rf__n_estimators': 500}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if frac<1: \n",
    "    for clf, label in zip([clf_ada, clf_rfc, clf_gbc, eclf], ['Adaboost', 'Random Forest', 'Gradient Boost', 'Ensemble']):\n",
    "        scores = cross_val_score(clf, X_secret, y_secret, cv=5, scoring='accuracy')\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=eclf.predict(X_test)\n",
    "assert len(y_pred)==300\n",
    "\n",
    "preds = pd.DataFrame({'ID': np.arange(300), 'label': y_pred})\n",
    "preds.to_csv('./preds_ag_'+str(datetime.datetime.now())+'.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: nlinks               Importance: 0.21\n",
      "Variable: hour                 Importance: 0.15\n",
      "Variable: length               Importance: 0.12\n",
      "Variable: rt                   Importance: 0.11\n",
      "Variable: nhashs               Importance: 0.1\n",
      "Variable: neu                  Importance: 0.08\n",
      "Variable: nats                 Importance: 0.06\n",
      "Variable: pos                  Importance: 0.06\n",
      "Variable: neg                  Importance: 0.05\n",
      "Variable: ncampaign            Importance: 0.04\n",
      "Variable: nself                Importance: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(clf_rfc.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: hour                 Importance: 0.53\n",
      "Variable: pos                  Importance: 0.11\n",
      "Variable: neg                  Importance: 0.1\n",
      "Variable: neu                  Importance: 0.08\n",
      "Variable: length               Importance: 0.07\n",
      "Variable: nlinks               Importance: 0.04\n",
      "Variable: rt                   Importance: 0.04\n",
      "Variable: nhashs               Importance: 0.02\n",
      "Variable: ncampaign            Importance: 0.01\n",
      "Variable: nats                 Importance: 0.0\n",
      "Variable: nself                Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(clf_ada.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: nlinks               Importance: 0.59\n",
      "Variable: rt                   Importance: 0.19\n",
      "Variable: hour                 Importance: 0.09\n",
      "Variable: nhashs               Importance: 0.06\n",
      "Variable: length               Importance: 0.03\n",
      "Variable: neg                  Importance: 0.02\n",
      "Variable: neu                  Importance: 0.01\n",
      "Variable: nats                 Importance: 0.0\n",
      "Variable: pos                  Importance: 0.0\n",
      "Variable: nself                Importance: 0.0\n",
      "Variable: ncampaign            Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(clf_gbc.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make wordclouds \n",
    "\n",
    "all_words = ' '.join(merge_rows(data_train[data_train['label']==1]['hashs']))\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110,colormap='plasma',  background_color=\"white\").generate(all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.savefig('plots/drumpf_wordcloud_hashs.jpeg', format='jpeg', dpi=200)\n",
    "\n",
    "all_words = ' '.join(merge_rows(data_train[data_train['label']==-1]['hashs']))\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110,colormap='plasma',  background_color=\"white\").generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.savefig('plots/minions_wordcloud_hashs.jpeg',format='jpeg', dpi=200)\n",
    "\n",
    "## make wordclouds \n",
    "\n",
    "all_words = ' '.join(merge_rows(data_train[data_train['label']==1]['ats']))\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, colormap='plasma', background_color=\"white\").generate(all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.savefig('plots/drumpf_wordcloud_ats.jpeg', format='jpeg', dpi=200)\n",
    "\n",
    "all_words = ' '.join(merge_rows(data_train[data_train['label']==-1]['ats']))\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, colormap='plasma', background_color=\"white\").generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.savefig('plots/minions_wordcloud_ats.jpeg',format='jpeg', dpi=200)\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(data_train[data_train['label']==1]['hour'], kde=True,color = 'blue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Drumpf')\n",
    "sns.distplot(data_train[data_train['label']==-1]['hour'], kde=True,color = 'red', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Minions')\n",
    "\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.title('Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('plots/time_density.jpeg', format='jpeg', dpi=200)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(data_train[data_train['label']==1]['length'], kde=True,color = 'blue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Drumpf')\n",
    "sns.distplot(data_train[data_train['label']==-1]['length'], kde=True,color = 'red', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Minions')\n",
    "\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.title('neu')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('plots/length_density.jpeg', format='jpeg', dpi=200)\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(data_train[data_train['label']==1]['pos'], kde=True,color = 'blue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Drumpf')\n",
    "sns.distplot(data_train[data_train['label']==-1]['pos'], kde=True,color = 'red', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Minions')\n",
    "\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.title('Positive emotions')\n",
    "plt.xlabel('Positivity')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.savefig('plots/pos_density.jpeg', format='jpeg', dpi=200)\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(data_train[data_train['label']==1]['neg'], kde=True,color = 'blue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Drumpf')\n",
    "sns.distplot(data_train[data_train['label']==-1]['neg'], kde=True,color = 'red', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Minions')\n",
    "\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.title('Negative emotions')\n",
    "plt.xlabel('Negativity')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.savefig('plots/neg_density.jpeg', format='jpeg', dpi=200)\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(data_train[data_train['label']==1]['neu'], kde=True,color = 'blue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Drumpf')\n",
    "sns.distplot(data_train[data_train['label']==-1]['neu'], kde=True,color = 'red', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4},label='Minions')\n",
    "\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.title('neu')\n",
    "plt.xlabel('Neutrality')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.savefig('plots/neu_density.jpeg', format='jpeg', dpi=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference in counted features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type    D  M\n",
      "------------------\n",
      "hashs:  83 449\n",
      "links:  55 418\n",
      "handles:  594 123\n",
      "allcaps:  414 229\n",
      "children:  69 168\n",
      "self:  119 1\n",
      "camp:  16 168\n"
     ]
    }
   ],
   "source": [
    "# is there a difference in number of links/hashtags used? \n",
    "\n",
    "hashs_drumpf = np.sum(gethashs(data_train[data_train['label'] == 1]['text']))\n",
    "hashs_minions = np.sum(gethashs(data_train[data_train['label'] == -1]['text']))\n",
    "\n",
    "links_drumpf = np.sum(getlinks(data_train[data_train['label'] == 1]['text']))\n",
    "links_minions = np.sum(getlinks(data_train[data_train['label'] == -1]['text']))\n",
    "\n",
    "ats_drumpf = np.sum(getats(data_train[data_train['label'] == 1]['text']))\n",
    "ats_minions = np.sum(getats(data_train[data_train['label'] == -1]['text']))\n",
    "\n",
    "allcaps_drumpf = np.sum(data_train[data_train['label'] == 1]['ncaps'])\n",
    "allcaps_minions = np.sum(data_train[data_train['label'] == -1]['ncaps'])\n",
    "\n",
    "children_drumpf = len(np.sum(getchildren(data_train[data_train['label'] == 1]['text'])))\n",
    "children_minions = len(np.sum(getchildren(data_train[data_train['label'] == -1]['text'])))\n",
    "\n",
    "self_drumpf = np.sum(data_train['nself'][data_train['label'] == 1])\n",
    "self_minions = np.sum(data_train['nself'][data_train['label'] == -1])\n",
    "\n",
    "camp_drumpf = np.sum(data_train['ncampaign'][data_train['label'] == 1])\n",
    "camp_minions = np.sum(data_train['ncampaign'][data_train['label'] == -1])\n",
    "\n",
    "## total number of hashtags and links \n",
    "print(\"Type  \", ' D ', 'M')\n",
    "print(\"------------------\")\n",
    "print('hashs: ', len(hashs_drumpf), len(hashs_minions))\n",
    "print('links: ', len(links_drumpf), len(links_minions))\n",
    "print('handles: ', len(ats_drumpf), len(ats_minions))\n",
    "print('allcaps: ', allcaps_drumpf, allcaps_minions)\n",
    "print('children: ', children_drumpf, children_minions)\n",
    "print('self: ', self_drumpf, self_minions)\n",
    "print('camp: ', camp_drumpf, camp_minions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectname",
   "language": "python",
   "name": "projectname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
